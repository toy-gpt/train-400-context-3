{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Toy GPT Repository","text":"<p>This repo is an example of a next-token prediction model, illustrating how language models are trained and used.</p>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li><code>README.md</code> in the repository root for the home page</li> <li><code>SETUP.md</code> for setup and workflow</li> <li><code>SE_MANIFEST.toml</code> for intent, scope, and declared training corpus</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>This page documents the public API of the package.</p> <p>Documented functions below are considered stable.</p>"},{"location":"api/#training-math","title":"Training Math","text":""},{"location":"api/#toy_gpt_train.math_training","title":"math_training","text":"<p>math_training.py - Mathematical utilities used during model training.</p> <p>This module contains reusable mathematical functions that appear throughout the training process of language models.</p> <p>Common themes: - These functions are not specific to any one model. - They are reused unchanged across unigram, bigram, and higher-context models. - Keeping them here avoids duplication and keeps training code readable.</p> <p>As models become more complex (embeddings, attention, batching), these core ideas remain the same.</p>"},{"location":"api/#toy_gpt_train.math_training.argmax","title":"argmax","text":"<pre><code>argmax(values: list[float]) -&gt; int\n</code></pre> <p>Return the index of the maximum value in a list.</p> Concept <p>argmax is the argument (index) at which a function reaches its maximum value.</p> In training and inference <ul> <li>A model outputs a probability distribution over possible next tokens.</li> <li>The token with the highest probability is the model's most confident prediction.</li> <li>argmax selects that token.</li> </ul> Example <p>values = [0.1, 0.7, 0.2] has index values of 0,1, 2 respectively. argmax(values) -&gt; 1 (since 0.7 is the largest value)</p> This is used for <ul> <li>Measuring accuracy during training</li> <li>Greedy decoding during inference</li> </ul> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>list[float]</code> <p>A list of numeric values (typically probabilities).</p> required <p>Returns:</p> Type Description <code>int</code> <p>The index of the largest value in the list.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the list is empty.</p>"},{"location":"api/#toy_gpt_train.math_training.cross_entropy_loss","title":"cross_entropy_loss","text":"<pre><code>cross_entropy_loss(\n    probs: list[float], target_id: int\n) -&gt; float\n</code></pre> <p>Compute cross-entropy loss for a single training example.</p> Cross-Entropy Loss <p>Cross-entropy measures how well a predicted probability distribution matches the true outcome.</p> <p>In next-token prediction: - The true distribution is \"one-hot\" which means we encode it as either 1 or 0:     - Probability = 1.0 for the correct next token     - Probability = 0.0 for all others - The model predicts a probability distribution over all tokens.</p> <p>Cross-entropy answers the question:     \"How well does the predicted probability distribution align with the true outcome?\"</p> Formula <p>loss = -log(p_correct)</p> <ul> <li>If the model assigns high probability to the correct token,   the loss is small.</li> <li>If the probability is near zero, the loss is large.</li> </ul> Numerical safety <p>log(0) is undefined, so we clamp probabilities to a small minimum (1e-12). This does not change learning behavior in practice, but prevents runtime errors.</p> In training <ul> <li>This loss value drives gradient descent.</li> <li>Lower loss means better predictions.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>probs</code> <code>list[float]</code> <p>A probability distribution over the vocabulary (sums to 1.0).</p> required <code>target_id</code> <code>int</code> <p>The integer ID of the correct next token.</p> required <p>Returns:</p> Type Description <code>float</code> <p>A non-negative floating-point loss value.</p> <code>float</code> <ul> <li>0.0 means a perfect prediction</li> </ul> <code>float</code> <ul> <li>Larger values indicate worse predictions</li> </ul>"},{"location":"api/#models","title":"Models","text":""},{"location":"api/#toy_gpt_train.c_model","title":"c_model","text":"<p>c_model.py - Simple model module.</p> <p>Defines a minimal next-token prediction model for a context-3 setting (uses three tokens in sequence as context).</p> <p>Responsibilities: - Represent a simple parameterized model that maps a   3-tuple of token IDs (prev2, prev1, current)   to a score for each token in the vocabulary. - Convert scores into probabilities using softmax. - Provide a forward pass (no training in this module).</p> <p>This model is intentionally simple: - one weight table (conceptually a 4D tensor: prev2 x prev1 x curr x next,   flattened for storage) - one forward computation - no learning here</p> <p>Training is handled in a different module.</p>"},{"location":"api/#toy_gpt_train.c_model.SimpleNextTokenModel","title":"SimpleNextTokenModel","text":"<p>A minimal next-token prediction model (context-3).</p>"},{"location":"api/#toy_gpt_train.c_model.SimpleNextTokenModel.__init__","title":"__init__","text":"<pre><code>__init__(vocab_size: int) -&gt; None\n</code></pre> <p>Initialize the model with random weights.</p>"},{"location":"api/#toy_gpt_train.c_model.SimpleNextTokenModel.forward","title":"forward","text":"<pre><code>forward(\n    prev2_id: int, prev1_id: int, current_id: int\n) -&gt; list[float]\n</code></pre> <p>Perform a forward pass to get next-token probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>prev2_id</code> <code>int</code> <p>Token ID of the token two positions before current.</p> required <code>prev1_id</code> <code>int</code> <p>Token ID of the token one position before current.</p> required <code>current_id</code> <code>int</code> <p>Token ID of the current token.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Probabilities for each token in the vocabulary.</p>"},{"location":"api/#toy_gpt_train.c_model.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Demonstrate a forward pass of the simple context-3 model.</p>"},{"location":"api/#training-pipeline","title":"Training Pipeline","text":""},{"location":"api/#toy_gpt_train.d_train","title":"d_train","text":"<p>d_train.py - Training loop module.</p> <p>Trains the SimpleNextTokenModel on a small token corpus using a context-3 window (three preceding tokens).</p> <p>Responsibilities: - Create ((token_{t-2}, token_{t-1}, token_t) -&gt; next_token) training pairs - Run a basic gradient-descent training loop - Track loss and accuracy per epoch - Write a CSV log of training progress - Write inspectable training artifacts (vocabulary, weights, embeddings, meta)</p> <p>Concepts: - softmax: converts raw scores into probabilities (so predictions sum to 1) - cross-entropy loss: measures how well predicted probabilities match the correct token - gradient descent: iterative weight updates to minimize loss   - think descending to find the bottom of a valley in a landscape   - where the valley floor corresponds to lower prediction error</p> <p>Notes: - This remains intentionally simple: no deep learning framework, no Transformer. - The model generalizes n-gram training by expanding the context window. - Training updates weight rows associated with the observed context-3 pattern. - token_embeddings.csv remains a derived visualization artifact;   learned embeddings are introduced in later stages.</p>"},{"location":"api/#toy_gpt_train.d_train.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Run a simple training demo end-to-end (context-3).</p>"},{"location":"api/#toy_gpt_train.d_train.make_training_pairs","title":"make_training_pairs","text":"<pre><code>make_training_pairs(\n    token_ids: list[int],\n) -&gt; list[Context3Pair]\n</code></pre> <p>Convert token IDs into ((t-2, t-1, t), next) training pairs.</p> Example <p>ids = [3, 1, 2, 4, 5] pairs = [((3, 1, 2), 4), ((1, 2, 4), 5)]</p>"},{"location":"api/#toy_gpt_train.d_train.row_labeler_context3","title":"row_labeler_context3","text":"<pre><code>row_labeler_context3(\n    vocab: VocabularyLike, vocab_size: int\n) -&gt; RowLabeler\n</code></pre> <p>Map a context-3 row index to a label like 'tok_{t-2}|tok_{t-1}|tok_t'.</p>"},{"location":"api/#toy_gpt_train.d_train.token_row_index_context3","title":"token_row_index_context3","text":"<pre><code>token_row_index_context3(\n    context_ids: Context3, vocab_size: int\n) -&gt; int\n</code></pre> <p>Return the row index for a context-3 token sequence.</p> Context order <p>(token_id_{t-2}, token_id_{t-1}, token_id_t)</p> Flattening scheme <p>row_index = a * vocab_size^2 + b * vocab_size + c</p> This is the context-3 analogue of <p>unigram: row = token_id bigram:  row = prev_id * vocab_size + curr_id</p>"},{"location":"api/#toy_gpt_train.d_train.train_model","title":"train_model","text":"<pre><code>train_model(\n    model: SimpleNextTokenModel,\n    pairs: list[Context3Pair],\n    learning_rate: float,\n    epochs: int,\n) -&gt; list[dict[str, float]]\n</code></pre> <p>Train the model using gradient descent on softmax cross-entropy (context-3).</p> Each example <p>context_ids = (token_id_{t-2}, token_id_{t-1}, token_id_t) target_id   = token_id_{t+1}</p> <p>Returns:</p> Type Description <code>list[dict[str, float]]</code> <p>A list of per-epoch metrics dictionaries (epoch, avg_loss, accuracy).</p>"},{"location":"api/#inference","title":"Inference","text":""},{"location":"api/#toy_gpt_train.e_infer","title":"e_infer","text":"<p>e_infer.py - Inference module (artifact-driven).</p> <p>Runs inference using previously saved training artifacts.</p> <p>Responsibilities: - Load inspectable training artifacts from artifacts/   - 00_meta.json   - 01_vocabulary.csv   - 02_model_weights.csv - Reconstruct a vocabulary-like interface and model weights - Generate tokens using greedy decoding (argmax) - Print top-k next-token probabilities for inspection</p> <p>Notes: - This module does NOT retrain by default. - If artifacts are missing, run d_train.py first. - Context-3 bootstrapping: generation starts from a single start token. To form the   first 3-token context, we use (start, start, start) as the initial context.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary","title":"ArtifactVocabulary  <code>dataclass</code>","text":"<p>Vocabulary reconstructed from artifacts/01_vocabulary.csv.</p> <p>Provides the same surface area used by inference: - vocab_size() - get_token_id() - get_id_token() - get_token_frequency()</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.get_id_token","title":"get_id_token","text":"<pre><code>get_id_token(idx: int) -&gt; str | None\n</code></pre> <p>Return the token for a given token ID, or None if not found.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.get_token_frequency","title":"get_token_frequency","text":"<pre><code>get_token_frequency(token: str) -&gt; int\n</code></pre> <p>Return the frequency count for a given token, or 0 if not found.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.get_token_id","title":"get_token_id","text":"<pre><code>get_token_id(token: str) -&gt; int | None\n</code></pre> <p>Return the token ID for a given token, or None if not found.</p>"},{"location":"api/#toy_gpt_train.e_infer.ArtifactVocabulary.vocab_size","title":"vocab_size","text":"<pre><code>vocab_size() -&gt; int\n</code></pre> <p>Return the total number of tokens in the vocabulary.</p>"},{"location":"api/#toy_gpt_train.e_infer.generate_tokens_context3","title":"generate_tokens_context3","text":"<pre><code>generate_tokens_context3(\n    model: SimpleNextTokenModel,\n    vocab: ArtifactVocabulary,\n    start_token: str,\n    num_tokens: int,\n) -&gt; list[str]\n</code></pre> <p>Generate tokens using a context-3 window (t-2, t-1, t).</p> Bootstrapping <p>If we only have one start token, we begin with:     (start, start, start) so that forward(previous2_id, previous1_id, current_id) is well-defined.</p>"},{"location":"api/#toy_gpt_train.e_infer.load_meta","title":"load_meta","text":"<pre><code>load_meta(path: Path) -&gt; JsonObject\n</code></pre> <p>Load 00_meta.json.</p>"},{"location":"api/#toy_gpt_train.e_infer.load_model_weights_csv","title":"load_model_weights_csv","text":"<pre><code>load_model_weights_csv(\n    path: Path, vocab_size: int, *, expected_rows: int\n) -&gt; list[list[float]]\n</code></pre> <p>Load 02_model_weights.csv -&gt; weights matrix.</p>"},{"location":"api/#toy_gpt_train.e_infer.load_vocabulary_csv","title":"load_vocabulary_csv","text":"<pre><code>load_vocabulary_csv(path: Path) -&gt; ArtifactVocabulary\n</code></pre> <p>Load 01_vocabulary.csv -&gt; ArtifactVocabulary.</p>"},{"location":"api/#toy_gpt_train.e_infer.main","title":"main","text":"<pre><code>main() -&gt; None\n</code></pre> <p>Run inference using saved training artifacts.</p>"},{"location":"api/#toy_gpt_train.e_infer.parse_args","title":"parse_args","text":"<pre><code>parse_args() -&gt; argparse.Namespace\n</code></pre> <p>Parse command-line arguments.</p>"},{"location":"api/#toy_gpt_train.e_infer.require_artifacts","title":"require_artifacts","text":"<pre><code>require_artifacts() -&gt; None\n</code></pre> <p>Fail fast with a helpful message if artifacts are missing.</p>"},{"location":"api/#toy_gpt_train.e_infer.top_k","title":"top_k","text":"<pre><code>top_k(\n    probs: list[float], k: int\n) -&gt; list[tuple[int, float]]\n</code></pre> <p>Return top-k (token_id, probability) pairs sorted by probability.</p>"},{"location":"api/#artifacts-and-io","title":"Artifacts and I/O","text":""},{"location":"api/#toy_gpt_train.io_artifacts","title":"io_artifacts","text":"<p>io.py - Input/output and training-artifact utilities used by the models.</p> <p>This module is responsible for persisting and describing the results of model training in a consistent, inspectable format.</p> <p>It does not perform training. It: - Writes artifacts produced by training (weights, vocabulary, logs, metadata) - Enforces a fixed repository layout for reproducibility - Provides small helper utilities shared across training and inference</p> <p>The directory structure is intentionally fixed: - artifacts/ contains all inspectable model outputs - corpus/ contains exactly one training text file - outputs/ contains training logs and diagnostics</p> <p>External callers should treat paths as implementation details and interact only through the functions provided here.</p>"},{"location":"api/#toy_gpt_train.io_artifacts--concepts","title":"Concepts","text":"<p>Artifact     A concrete file written to disk that captures some aspect of training.     In this project, artifacts are designed to be:     - Human-readable (CSV / JSON)     - Stable across model variants (unigram, bigram, context-3, etc.)     - Reusable by inference without retraining</p> <p>Epoch     One epoch is one complete pass through all training examples.     Training typically consists of multiple epochs so the model can     gradually improve its predictions by repeatedly adjusting weights.</p> <p>Training Log     A CSV file recording per-epoch metrics such as:     - average loss     - accuracy     This allows learning behavior to be inspected after training.</p> <p>Vocabulary     A mapping between token strings and integer token IDs.     The vocabulary defines:     - the size of the model output space     - the meaning of each row and column in the weight tables</p> <p>Row Labeler     A small function that maps a numeric row index in the model's weight table     to a human-readable label.     For example, as the number of context tokens increases, the row labeler     produces context strings such as:     - unigram:        \"cat\"     - bigram:         \"the|cat\"     - context-3:      \"the|black|cat\"</p> <pre><code>Row labels are written into CSV artifacts to make model structure visible.\n</code></pre> <p>Model Weights     Numeric parameters learned during training.     Conceptually:     - each row corresponds to an input context     - each column corresponds to a possible next token     Weights are written verbatim so learning can be inspected or reused.</p> <p>Token Embeddings (Derived)     A simple 2D projection derived from model weights for visualization.     These are not learned embeddings yet.     In later stages (500+), embeddings become first-class learned parameters.</p> <p>Reproducibility Metadata     The 00_meta.json file records:     - which corpus was used     - how it was hashed     - which model variant was trained     - what training settings were applied     This allows results to be traced and compared across runs and repositories.</p>"},{"location":"api/#toy_gpt_train.io_artifacts--design-notes","title":"Design Notes","text":"<ul> <li>This module is shared unchanged across model levels (100-400).</li> <li>More advanced pipelines (embeddings, attention, batching) build on   the same artifact-writing concepts.</li> <li>Centralizing I/O logic prevents drift across repositories   and keeps training code focused on learning.</li> </ul>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike","title":"VocabularyLike","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for vocabulary-like objects used in training and artifacts.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.get_id_token","title":"get_id_token","text":"<pre><code>get_id_token(idx: int) -&gt; str | None\n</code></pre> <p>Return the token string for a given integer ID, or None if not found.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.get_token_frequency","title":"get_token_frequency","text":"<pre><code>get_token_frequency(token: str) -&gt; int\n</code></pre> <p>Return the frequency count for a given token.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.get_token_id","title":"get_token_id","text":"<pre><code>get_token_id(token: str) -&gt; int | None\n</code></pre> <p>Return the integer ID for a given token, or None if not found.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.VocabularyLike.vocab_size","title":"vocab_size","text":"<pre><code>vocab_size() -&gt; int\n</code></pre> <p>Return the total number of unique tokens in the vocabulary.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.find_single_corpus_file","title":"find_single_corpus_file","text":"<pre><code>find_single_corpus_file(corpus_dir: Path) -&gt; Path\n</code></pre> <p>Find the single corpus file in corpus/ (same rule as SimpleTokenizer).</p>"},{"location":"api/#toy_gpt_train.io_artifacts.repo_name_from_base_dir","title":"repo_name_from_base_dir","text":"<pre><code>repo_name_from_base_dir(base_dir: Path) -&gt; str\n</code></pre> <p>Infer repository name from base directory.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.sha256_of_bytes","title":"sha256_of_bytes","text":"<pre><code>sha256_of_bytes(data: bytes) -&gt; str\n</code></pre> <p>Return hex SHA-256 digest for given bytes.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.sha256_of_file","title":"sha256_of_file","text":"<pre><code>sha256_of_file(path: Path) -&gt; str\n</code></pre> <p>Return hex SHA-256 digest for a file.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.write_artifacts","title":"write_artifacts","text":"<pre><code>write_artifacts(\n    *,\n    base_dir: Path,\n    corpus_path: Path,\n    vocab: VocabularyLike,\n    model: SimpleNextTokenModel,\n    model_kind: str,\n    learning_rate: float,\n    epochs: int,\n    row_labeler: RowLabeler,\n) -&gt; None\n</code></pre> <p>Write all training artifacts to artifacts/.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>Path</code> <p>Repository base directory.</p> required <code>corpus_path</code> <code>Path</code> <p>Corpus file used for training.</p> required <code>vocab</code> <code>VocabularyLike</code> <p>VocabularyLike instance.</p> required <code>model</code> <code>SimpleNextTokenModel</code> <p>Trained model (weights already updated).</p> required <code>model_kind</code> <code>str</code> <p>Human-readable model kind (e.g., \"unigram\", \"bigram\").</p> required <code>learning_rate</code> <code>float</code> <p>Training learning rate.</p> required <code>epochs</code> <code>int</code> <p>Number of training passes.</p> required <code>row_labeler</code> <code>RowLabeler</code> <p>Function that maps a model weight-row index to a label written in the first column of 02_model_weights.csv.</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_meta_json","title":"write_meta_json","text":"<pre><code>write_meta_json(\n    path: Path,\n    *,\n    base_dir: Path,\n    corpus_path: Path,\n    vocab_size: int,\n    model_kind: str,\n    learning_rate: float,\n    epochs: int,\n) -&gt; None\n</code></pre> <p>Write 00_meta.json describing corpus, model, and training settings.</p> <p>This file is the authoritative, human-readable summary of a training run. It records: - what corpus was used - what model architecture was trained - how training was configured - which artifacts were produced</p> <p>The intent is transparency and reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output JSON path (artifacts/00_meta.json).</p> required <code>base_dir</code> <code>Path</code> <p>Repository base directory.</p> required <code>corpus_path</code> <code>Path</code> <p>Corpus file used for training.</p> required <code>vocab_size</code> <code>int</code> <p>Number of unique tokens.</p> required <code>model_kind</code> <code>str</code> <p>Human-readable model kind (e.g., \"unigram\", \"bigram\", \"context2\", \"context3\").</p> required <code>learning_rate</code> <code>float</code> <p>Training learning rate.</p> required <code>epochs</code> <code>int</code> <p>Number of epochs (full passes over the training pairs).</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_model_weights_csv","title":"write_model_weights_csv","text":"<pre><code>write_model_weights_csv(\n    path: Path,\n    vocab: VocabularyLike,\n    model: SimpleNextTokenModel,\n    *,\n    row_labeler: RowLabeler,\n) -&gt; None\n</code></pre> <p>Write 02_model_weights.csv with token-labeled columns.</p> Shape <ul> <li>first column: input_token</li> <li>remaining columns: one per output token (header names are tokens)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output CSV path.</p> required <code>vocab</code> <code>VocabularyLike</code> <p>Vocabulary instance (must provide vocab_size(), get_id_token()).</p> required <code>model</code> <code>SimpleNextTokenModel</code> <p>Trained model (must provide vocab_size and weights).</p> required <code>row_labeler</code> <code>RowLabeler</code> <p>Function that maps a model weight-row index to a label written in the first column.</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_token_embeddings_csv","title":"write_token_embeddings_csv","text":"<pre><code>write_token_embeddings_csv(\n    path: Path,\n    model: SimpleNextTokenModel,\n    *,\n    row_labeler: RowLabeler,\n) -&gt; None\n</code></pre> <p>Write 03_token_embeddings.csv as a simple 2D projection for plotting.</p>"},{"location":"api/#toy_gpt_train.io_artifacts.write_training_log","title":"write_training_log","text":"<pre><code>write_training_log(\n    path: Path, history: list[dict[str, float]]\n) -&gt; None\n</code></pre> <p>Write per-epoch training metrics to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output file path.</p> required <code>history</code> <code>list[dict[str, float]]</code> <p>List of per-epoch metrics dictionaries.</p> required"},{"location":"api/#toy_gpt_train.io_artifacts.write_vocabulary_csv","title":"write_vocabulary_csv","text":"<pre><code>write_vocabulary_csv(\n    path: Path, vocab: VocabularyLike\n) -&gt; None\n</code></pre> <p>Write 01_vocabulary.csv: token_id, token, frequency.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Output CSV path.</p> required <code>vocab</code> <code>VocabularyLike</code> <p>Vocabulary instance (must provide vocab_size(), get_id_token(), get_token_frequency()).</p> required"},{"location":"api/#marker-file-pytyped","title":"Marker File: py.typed","text":"<p>This package includes a <code>py.typed</code> marker file as defined by PEP 561.</p> <ul> <li>Type checkers (Pyright, Mypy) trust inline type hints in installed packages only when this marker is present.</li> <li>The file may be empty; comments are allowed.</li> </ul>"}]}